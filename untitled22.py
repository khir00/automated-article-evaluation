# -*- coding: utf-8 -*-
"""Untitled22.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-4y-5RJgjbY8ioo3xZvby3WiDxmYTVdP
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
df = pd.read_csv('/content/drive/MyDrive/ASAP2_train_sourcetexts.csv', encoding='ISO-8859-1')
print(df.head())

print(df.info())
print(df.describe())

!pip install -U language-tool-python

!sudo apt update
!sudo apt install openjdk-17-jdk -y
!update-alternatives --install /usr/bin/java java /usr/lib/jvm/java-17-openjdk-amd64/bin/java 1
!update-alternatives --set java /usr/lib/jvm/java-17-openjdk-amd64/bin/java
!java -version

import nltk

nltk.download('punkt')
nltk.download('punkt_tab')

import pandas as pd
import re
import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
import language_tool_python
from sklearn.preprocessing import StandardScaler
from sentence_transformers import SentenceTransformer
import numpy as np
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Masking, Bidirectional, LSTM, Dropout, Concatenate, Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.metrics import mean_absolute_error, mean_squared_error, cohen_kappa_score
import matplotlib.pyplot as plt

df = df[['essay_id', 'score', 'full_text', 'assignment', 'prompt_name', 'ell_status', 'source_text_1']].copy()

def clean_text(text):
    text = re.sub(r'<[^>]+>', '', str(text))
    text = re.sub(r'[^\x00-\x7F]+', ' ', text)
    text = ' '.join(text.split())
    return text.strip()
df['clean_text'] = df['full_text'].apply(clean_text)

tool = language_tool_python.LanguageTool('en-US')

def extract_features(row):
    text = row['clean_text']
    ell = 1 if row['ell_status'] == 'ELL' else 0
    num_words = len(word_tokenize(text))
    num_sentences = len(sent_tokenize(text))
    num_errors = len(tool.check(text))
    return pd.Series([num_words, num_sentences, ell, num_errors])

df[['num_words', 'num_sentences', 'is_ell', 'grammar_errors']] = df.apply(extract_features, axis=1)

sbert = SentenceTransformer('all-MiniLM-L6-v2')

def get_sbert_sequence(text, max_sentences=20):
    sentences = sent_tokenize(text)
    sentences = sentences[:max_sentences]
    embeddings = sbert.encode(sentences)

    if len(embeddings) < max_sentences:
        padding = np.zeros((max_sentences - len(embeddings), embeddings.shape[1]))
        embeddings = np.vstack((embeddings, padding))

    return embeddings

X_seq_full = np.array(df['clean_text'].apply(get_sbert_sequence).tolist()).astype('float32')

X_feat_full = df[['num_words', 'num_sentences', 'is_ell', 'grammar_errors']].values.astype('float32')

y_full = df['score'].values.astype('float32')

X_seq_train, X_seq_test, X_feat_train_raw, X_feat_test_raw, y_train, y_test = train_test_split(
    X_seq_full, X_feat_full, y_full, test_size=0.2, random_state=42
)

scaler = StandardScaler()
X_feat_train = scaler.fit_transform(X_feat_train_raw)
X_feat_test = scaler.transform(X_feat_test_raw)

sequence_length = X_seq_train.shape[1]
input_dim = X_seq_train.shape[2]
feature_dim = X_feat_train.shape[1]

input_seq = Input(shape=(sequence_length, input_dim))
x = Masking(mask_value=0.0)(input_seq)
x = Bidirectional(LSTM(128, return_sequences=True))(x)
x = Dropout(0.3)(x)
x = Bidirectional(LSTM(64))(x)
x = Dropout(0.3)(x)

input_feat = Input(shape=(feature_dim,))
f = Dense(32, activation='relu')(input_feat)

combined = Concatenate()([x, f])
combined = Dense(64, activation='relu')(combined)
combined = Dropout(0.2)(combined)
output = Dense(1, activation='linear')(combined)

model = Model(inputs=[input_seq, input_feat], outputs=output)
model.compile(optimizer=Adam(learning_rate=0.0005), loss='mse', metrics=['mae'])

model.summary()

early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

history = model.fit(
    [X_seq_train, X_feat_train], y_train,
    epochs=20,
    batch_size=16,
    validation_split=0.1,
    callbacks=[early_stop]
)

preds= model.predict([X_seq_test, X_feat_test]).flatten()

mae = mean_absolute_error(y_test, preds)
rmse = np.sqrt(mean_squared_error(y_test, preds))

valid_scores = sorted(df['score'].unique())
def round_to_valid(p):
    return min(valid_scores, key=lambda x: abs(x - p))

preds_rounded = np.array([round_to_valid(p) for p in preds])
y_test_rounded = np.array([round_to_valid(v) for v in y_test])

qwk = cohen_kappa_score(preds_rounded, y_test_rounded, weights='quadratic')

print(f"✅ MAE  : {mae:.4f}")
print(f"✅ RMSE : {rmse:.4f}")
print(f"✅ QWK  : {qwk:.4f}")

comparison_df = pd.DataFrame({
      'Actual':  y_test,
      'Predicted':  preds,
      'Predicted_rounded':  preds_rounded
})
print("\nFirst 20 compare between Actual & Predicted")
print(comparison_df.head(20))

plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.title('Training vs Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.grid()
plt.show()

import matplotlib.pyplot as plt

plt.scatter(y_test, preds, alpha=0.5)
plt.xlabel("Actual Value")
plt.ylabel("Predicted Value")
plt.title("Pre x Act")
plt.grid()
plt.show()

